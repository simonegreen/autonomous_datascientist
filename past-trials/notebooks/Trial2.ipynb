{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f442002",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c2e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c02a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pip in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (24.3.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7652df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\python312\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.55.0-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.8 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Python312\\\\share'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d895795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\n",
      "  Downloading scikit-learn-extra-0.3.0.tar.gz (818 kB)\n",
      "     ---------------------------------------- 0.0/819.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 819.0/819.0 kB 9.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python312\\lib\\site-packages (from scikit-learn-extra) (2.1.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python312\\lib\\site-packages (from scikit-learn-extra) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\python312\\lib\\site-packages (from scikit-learn-extra) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.5.0)\n",
      "Building wheels for collected packages: scikit-learn-extra\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml): started\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml): finished with status 'error'\n",
      "Failed to build scikit-learn-extra\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for scikit-learn-extra (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [60 lines of output]\n",
      "      E:\\Temp\\pip-build-env-hm3epjds\\overlay\\Lib\\site-packages\\setuptools\\dist.py:488: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2025-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\benchmarks\n",
      "      copying benchmarks\\bench_rbfsampler_fastfood.py -> build\\lib.win-amd64-cpython-312\\benchmarks\n",
      "      copying benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\benchmarks\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\n",
      "      copying sklearn_extra\\_version.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\n",
      "      copying sklearn_extra\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\n",
      "      creating build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\eigenpro_plot_mnist.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\eigenpro_plot_noisy_mnist.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\eigenpro_plot_synthetic.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\robust_plot_synthetic.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\__init__.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      copying sklearn_extra\\cluster\\_commonnn.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      copying sklearn_extra\\cluster\\_k_medoids.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      copying sklearn_extra\\cluster\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      copying sklearn_extra\\kernel_approximation\\test_fastfood.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      copying sklearn_extra\\kernel_approximation\\_fastfood.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      copying sklearn_extra\\kernel_approximation\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\n",
      "      copying sklearn_extra\\kernel_methods\\_eigenpro.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\n",
      "      copying sklearn_extra\\kernel_methods\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      copying sklearn_extra\\robust\\mean_estimators.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      copying sklearn_extra\\robust\\robust_weighted_estimator.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      copying sklearn_extra\\robust\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\tests\n",
      "      copying sklearn_extra\\tests\\test_common.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\tests\n",
      "      copying sklearn_extra\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\tests\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      copying sklearn_extra\\cluster\\tests\\test_commonnn.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      copying sklearn_extra\\cluster\\tests\\test_k_medoids.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      copying sklearn_extra\\cluster\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\\tests\n",
      "      copying sklearn_extra\\kernel_approximation\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\\tests\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\\tests\n",
      "      copying sklearn_extra\\kernel_methods\\tests\\test_eigenpro.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\\tests\n",
      "      copying sklearn_extra\\kernel_methods\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\\tests\n",
      "      running build_ext\n",
      "      building 'sklearn_extra.utils._cyfht' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-learn-extra\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-learn-extra)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f00d1a",
   "metadata": {},
   "source": [
    "### Helper Functions: Algorithms & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5c78b",
   "metadata": {},
   "source": [
    "**Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8fe4e",
   "metadata": {},
   "source": [
    "DONE: add a \"mode\" argument to each algorithm that, if mode = 1 the cluster labelling is output and if mode = 0 the silhouette coefficient is output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbfd14e3-20b2-4404-a2ea-ff8f7e5aa83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for KMeans\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def kmeans_clustering(samples,mode,  n_clusters=2, max_iter=300):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on the input samples\n",
    "    \n",
    "    Parameters:\n",
    "        samples: array-like, shape (n_samples, n_features)\n",
    "        n_clusters: int, number of clusters (default=2)\n",
    "        max_iter: int, maximum iterations (default=300)\n",
    "    \n",
    "    Returns:\n",
    "        silhouette_coef: silhouette coefficient score\n",
    "    \"\"\"\n",
    "    k_means = KMeans(n_clusters=n_clusters, max_iter=max_iter)\n",
    "    k_means.fit(samples)\n",
    "    if mode == 0:\n",
    "        try:\n",
    "            silhouette_coef = silhouette_score(samples, k_means.labels_, metric='euclidean')\n",
    "        except ValueError:\n",
    "            silhouette_coef = 0  # Assigning lowest score if clustering fails\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d63cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM Clustering Code\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def em_clustering(selected_features, mode, n_clusters=2):\n",
    "    \"\"\"\n",
    "    Perform EM Clustering on selected features and return silhouette score.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Silhouette score of the clustering (-1 if clustering fails)\n",
    "    \"\"\"\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize and fit the EM model\n",
    "    em_model = GaussianMixture(\n",
    "        n_components=n_clusters,\n",
    "        random_state=0, #THOUGHTS: We can improve this later to have an array of seeds to select from to observe variations\n",
    "        n_init=10  # Multiple initializations to avoid local optima\n",
    "    )\n",
    "    \n",
    "   \n",
    "    try:\n",
    "        # Fit the model and get cluster assignments\n",
    "        em_model.fit(X_scaled)\n",
    "        labels = em_model.predict(X_scaled)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except Exception as e:\n",
    "        #print(f\"Clustering failed: {str(e)}\")\n",
    "        silhouette_coef = 0  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9944db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Detection method: \n",
    "# I put 'optimization part' in 'DBSCAN_Optimization_Code.ipynb' file. \n",
    "# We can use optimization after initial run to do a comparison and analysis in our paper to show improvements.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TO-DO: keep -1 --> they will be its own cluster\n",
    "def dbscan_clustering(selected_features, mode, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on selected features\n",
    "    \n",
    "    Parameters:\n",
    "    selected_features : pandas DataFrame\n",
    "        The features selected for clustering\n",
    "    eps : float\n",
    "        The maximum distance between two samples for them to be considered neighbors\n",
    "    min_samples : int\n",
    "        The number of samples in a neighborhood for a point to be considered a core point\n",
    "        \n",
    "    Returns:\n",
    "    float : silhouette coefficient\n",
    "    dict : additional clustering information\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize and fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Get number of clusters (excluding noise points which are labeled -1, K Medoids does not have noise points)\n",
    "    n_clusters = len(set(labels))\n",
    "\n",
    "    if -1 in labels:\n",
    "        labels[labels == -1] = n_clusters - 1\n",
    "    \n",
    "    # calculate silhouette score if more than one cluster and  noise points\n",
    "    if n_clusters > 1:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    else:\n",
    "        silhouette_coef = 0  # Assign lowest score if clustering fails\n",
    "\n",
    "    \n",
    "    # NOTE: -- Uncomment when we analyze and optimize ---- Additional clustering information\n",
    "    # info = {\n",
    "    #     'n_clusters': n_clusters,\n",
    "    #     'n_noise': list(labels).count(-1),\n",
    "    #     'labels': labels,\n",
    "    #     'cluster_sizes': pd.Series(labels).value_counts().to_dict()\n",
    "    # }\n",
    "    \n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16776843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for K Medoids\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def kmedoids_clustering(selected_features, mode, n_clusters=2):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "     # Initialize and fit the K-Medoids model\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters, method='pam', max_iter=1500, random_state=0)\n",
    "    labels = kmedoids.fit_predict(X_scaled)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except ValueError:\n",
    "        silhouette_coef = 0  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078cfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for Mean Shift\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def meanshift_clustering(selected_features, mode, bandwidth=None):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize and fit the Mean Shift model\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    labels = meanshift.fit_predict(X_scaled)\n",
    "\n",
    "    # Check the number of clusters determined \n",
    "    n_clusters = len(np.unique(labels))\n",
    "    #print(f\"Number of clusters found: {n_clusters}\")\n",
    "    \n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except ValueError:\n",
    "        silhouette_coef = 0  # Assign lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d2115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_features(state):\n",
    "  state_bin = bin(state)\n",
    "  #print(state_bin)\n",
    "  state_bin_arr = np.array([b for b in state_bin[2:]])\n",
    "  # pad with zeros\n",
    "  diff = 10 - len(state_bin_arr)\n",
    "  padded_arr = np.insert(state_bin_arr, 0, ['0' for i in range(diff)])\n",
    "  #(padded_arr)\n",
    "  # identify which indexes are 1\n",
    "  idx = (np.where(padded_arr == '1')[0]).tolist()\n",
    "  #print(idx)\n",
    "  # select feature headings\n",
    "  selected_features = original_features.iloc[:,idx]\n",
    "  features = selected_features.columns.tolist()\n",
    "  res = f\"Features Used: {features}\"\n",
    "  return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740d094",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action', 'behavior_name', 'event_id:4', 'exploit_available', 'file_signature_status', 'network_interface', 'patch_name', 'severity', 'tcp_flags', 'vulnerability_solution']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = np.array(['action', 'availability', 'device_type:1', 'direction',\n",
    "#        'event_type', 'interface_status', 'patch_description',\n",
    "#        'patch_status', 'severity:1', 'traffic_direction'])\n",
    "\n",
    "# Replace feature list input with selected features\n",
    "features = np.array(['action', 'behavior_name', 'event_id:4', 'exploit_available',\n",
    " 'file_signature_status', 'network_interface', 'patch_name', 'severity',\n",
    " 'tcp_flags', 'vulnerability_solution'])\n",
    "\n",
    "features = features.tolist()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action', 'behavior_name', 'event_id:4', 'exploit_available', 'file_signature_status', 'network_interface', 'patch_name', 'severity', 'tcp_flags', 'vulnerability_solution']\n",
      "   action  behavior_name  event_id:4  exploit_available  \\\n",
      "0       0              0           0                  0   \n",
      "1       0              0           1                  0   \n",
      "2       0              0           2                  0   \n",
      "3       0              0           3                  0   \n",
      "4       0              0           4                  0   \n",
      "5       0              0           5                  0   \n",
      "6       0              0           6                  0   \n",
      "7       0              0           7                  0   \n",
      "8       0              0           8                  0   \n",
      "9       0              0           9                  0   \n",
      "\n",
      "   file_signature_status  network_interface  patch_name  severity  tcp_flags  \\\n",
      "0                      0                  0           0         0          0   \n",
      "1                      0                  0           1         0          0   \n",
      "2                      0                  0           2         0          0   \n",
      "3                      0                  0           3         0          0   \n",
      "4                      0                  0           4         0          0   \n",
      "5                      0                  0           5         0          0   \n",
      "6                      0                  0           6         0          0   \n",
      "7                      0                  0           7         0          0   \n",
      "8                      0                  0           8         0          0   \n",
      "9                      0                  0           9         0          0   \n",
      "\n",
      "   vulnerability_solution  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "5                       0  \n",
      "6                       0  \n",
      "7                       0  \n",
      "8                       0  \n",
      "9                       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# FEATURES = {0: 'avg_bytes_sent', 1: 'avg_bytes_received', 2: 'avg_packets_transferred', \n",
    "#   3: 'avg_flow_duration', 4: 'recent_tcp_flags', 5: 'recent_protocol', 6: 'avg_cpu_usage', \n",
    "#   7: 'avg_memory_usage', 8: 'avg_disk_usage', 9: 'avg_uptime'}\n",
    "FEATURES = {k:str(v) for k,v in zip(range(len(features)), features) }\n",
    "#print(FEATURES)\n",
    "data = pd.read_csv(\"cleaned_no_aggregation.csv\") # UPDATE NEW DATA HERE\n",
    "#print(data.head(10))\n",
    "\n",
    "\n",
    "ALGORITHMS = {4: 'K-Means', 1: 'Mean Shift', 2: 'K-Mediods', 3: 'EM Clustering', 0: 'DBSCAN Clustering'}\n",
    "NUM_ALG = len(ALGORITHMS)\n",
    "original_features = data[features].copy(deep = True)\n",
    "print(original_features.head(10))\n",
    "ips = data['source_ip_main']\n",
    "#print(original_features.columns)\n",
    "#print(ips.head(10))\n",
    "\n",
    "def algorithm_prep(state, action, mode):\n",
    "  # convert state to binary\n",
    "  state_bin = bin(state)\n",
    "  #print(state_bin)\n",
    "  state_bin_arr = np.array([b for b in state_bin[2:]])\n",
    "  # pad with zeros\n",
    "  diff = 10 - len(state_bin_arr)\n",
    "  padded_arr = np.insert(state_bin_arr, 0, ['0' for i in range(diff)])\n",
    "  #(padded_arr)\n",
    "  # identify which indexes are 1\n",
    "  idx = (np.where(padded_arr == '1')[0]).tolist()\n",
    "  #print(idx)\n",
    "  # select feature headings\n",
    "  selected_features = original_features.iloc[:,idx]\n",
    "  #print(selected_features.head(10))\n",
    "  # select algorithm\n",
    "  # algo = action\n",
    "  # prep correct data - done\n",
    "  \n",
    "  # call algorithm function\n",
    "  out = None\n",
    "  #print('algorithm:',ALGORITHMS[action])\n",
    "\n",
    "  # if mode = 0, output is the silhouette coefficient\n",
    "  # if mode = 1, output is the cluster labelling\n",
    "  match action:   \n",
    "    case 0:\n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = dbscan_clustering(selected_features, mode)\n",
    "    case 1: \n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = meanshift_clustering(selected_features, mode)\n",
    "    case 2:\n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = kmedoids_clustering(selected_features, mode)\n",
    "    case 3: \n",
    "      out = em_clustering(selected_features, mode)\n",
    "    case 4:\n",
    "      out = kmeans_clustering(selected_features, mode)\n",
    "      \n",
    "  # return silhouette from algorithm function\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148db5c",
   "metadata": {},
   "source": [
    "##### Sampling (Test Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed306bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "# idx = []\n",
    "# anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "# print(len(anomalies))\n",
    "perc_values = labelled_data['cluster'].value_counts(ascending=True)\n",
    "print(perc_values)\n",
    "print(\"flipped:\",perc_values.values)\n",
    "print(percentages)\n",
    "idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "print(idx)\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "print(len(anomalies))\n",
    "perc_values = labelled_data['cluster'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e31d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "[1]\n",
      "IP 192.168.0.248 is a potential anomaly\n",
      "IP 192.168.0.78 is a potential anomaly\n"
     ]
    }
   ],
   "source": [
    "label_test = algorithm_prep(256, 4,1)\n",
    "labelled_data = data.copy()\n",
    "labelled_data['cluster'] = label_test\n",
    "labelled_data.head(10)\n",
    "\n",
    "num_clusters = labelled_data['cluster'].nunique()\n",
    "vals = labelled_data['cluster'].value_counts().values\n",
    "print(labelled_data.shape[0])\n",
    "vals = vals / labelled_data.shape[0]\n",
    "idx = (np.where(vals <= 0.1)[0]).tolist()\n",
    "print(idx)\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "#print(anomalies)\n",
    "for i in anomalies['source_ip']:\n",
    "    print(f\"IP {i} is a potential anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045098f",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beedbfe-7d34-401e-b99f-e48e93937a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q  :\n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [1.         1.         3.3616     1.         2.42979049]\n",
      " [1.         2.952      2.952      1.8        2.94383239]\n",
      " ...\n",
      " [0.84751763 2.98925457 2.45613766 1.64288253 2.56068915]\n",
      " [1.6476037  2.98925457 1.30434306 1.6476037  2.56068888]\n",
      " [1.65653915 2.98925457 2.44953844 1.65648409 2.56068699]]\n",
      "Normed Q :\n",
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [27.10555989 27.10555989 91.11805013 27.10555989 65.86083158]\n",
      " [27.10555989 80.0156128  80.0156128  48.79000781 79.79422515]\n",
      " ...\n",
      " [22.97243997 81.02541874 66.57498654 44.53125079 69.40891316]\n",
      " [44.65922067 81.02541874 35.35494898 44.65922067 69.4089058 ]\n",
      " [44.9014211  81.02541874 66.39611095 44.89992861 69.40885461]]\n",
      "\n",
      "max value located at (array([18, 64, 65]), array([2, 2, 2]))\n",
      "\n",
      "Using algorithm K-Mediods and Features Used: ['network_interface', 'tcp_flags'], max value is: 100.0\n",
      "\n",
      "IP 192.168.0.226 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.226 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.226 is a potential anomaly\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process (MDP) - The Bellman equations adapted to\n",
    "# Q Learning.Reinforcement Learning with the Q action-value(reward) function.\n",
    "# Copyright 2018 Denis Rothman MIT License. See LICENSE.\n",
    "import numpy as ql\n",
    "# R is The Reward Matrix for each state\n",
    "# 1024 configurations of the 10 features --> 2^10\n",
    "# 5 algorithms\n",
    "num_configs = 2 ** len(FEATURES)\n",
    "R = ql.matrix(ql.zeros([num_configs,5]))\n",
    "\n",
    "# Q is the Learning Matrix in which rewards will be learned/stored\n",
    "Q = ql.matrix(ql.zeros([num_configs,5]))\n",
    "\n",
    "# Gamma : It's a form of penalty or uncertainty for learning\n",
    "# If the value is 1 , the rewards would be too high.\n",
    "# This way the system knows it is learning.\n",
    "gamma = 0.8\n",
    "\n",
    "# agent_s_state. The agent the name of the system calculating\n",
    "# s is the state the agent is going from and s' the state it's going to\n",
    "# this state can be random or it can be chosen as long as the rest of the choices\n",
    "# are not determined. Randomness is part of this stochastic process\n",
    "# 1) DONE: decide if starting state is random or a specific state\n",
    "agent_s_state = 1\n",
    "\n",
    "# The possible \"a\" actions when the agent is in a given state\n",
    "def possible_actions(state):\n",
    "    # 2) DONE: we should check Q, not R because R is never modified\n",
    "    current_state_row = Q[state,]\n",
    "    # 3) DONE: this should pick valid actions based on what we have not visited\n",
    "    possible_act = ql.where(current_state_row == 0)[1]\n",
    "    return possible_act\n",
    "\n",
    "# Get available actions in the current state\n",
    "PossibleAction = possible_actions(agent_s_state)\n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def ActionChoice(available_actions_range):\n",
    "    if(sum(PossibleAction)>0):\n",
    "        next_action = int(ql.random.choice(PossibleAction,1)[0])\n",
    "    if(sum(PossibleAction)<=0):\n",
    "        next_action = int(np.random.choice(NUM_ALG,1)[0])\n",
    "    return next_action\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = ActionChoice(PossibleAction)\n",
    "\n",
    "# A version of Bellman's equation for reinforcement learning using the Q function\n",
    "# This reinforcement algorithm is a memoryless process\n",
    "# The transition function T from one state to another\n",
    "# is not in the equation below.  T is done by the random choice above\n",
    "\n",
    "def reward(current_state, action, gamma):\n",
    "    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n",
    "\n",
    "    if Max_State.shape[0] > 1:\n",
    "        Max_State = int(ql.random.choice(Max_State, size = 1)[0])\n",
    "    else:\n",
    "        Max_State = int(Max_State[0])\n",
    "\n",
    "    # 5) DONE: we think this is a typo and action/Max_State should be switched. \n",
    "    # MaxValue = Q[action, Max_State]\n",
    "    MaxValue = Q[Max_State, action]\n",
    "\n",
    "    # 6) DONE: call function to run ML algorithm using the value of action. this will\n",
    "    # run the algorithm using the features from current_state, create clusters,\n",
    "    # and calculate the silhouette value.\n",
    "    silhouette_co = algorithm_prep(current_state, action, 0) \n",
    "    \n",
    "    # Bellman's MDP based Q function\n",
    "    # 7) DONE: instead of getting a value from R, we add the silhouette value to gamma * MaxValue\n",
    "    # Q[current_state, action] = R[current_state, action] + gamma * MaxValue\n",
    "    Q[current_state, action] = silhouette_co + gamma * MaxValue\n",
    "\n",
    "\n",
    "# Rewarding Q matrix\n",
    "reward(agent_s_state,action,gamma)\n",
    "\n",
    "\n",
    "# Leraning over n iterations depending on the convergence of the system\n",
    "# A convergence function can replace the systematic repeating of the process\n",
    "# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n",
    "# previous episode\n",
    "for i in range(10000):\n",
    "    # select a random new state (configuration of features)\n",
    "    current_state = ql.random.randint(1, int(Q.shape[0]))\n",
    "    PossibleAction = possible_actions(current_state)\n",
    "    action = ActionChoice(PossibleAction)\n",
    "    reward(current_state,action,gamma)\n",
    "    \n",
    "# Displaying Q before the norm of Q phase\n",
    "print(\"Q  :\")\n",
    "print(Q)\n",
    "\n",
    "# Norm of Q\n",
    "print(\"Normed Q :\")\n",
    "print(Q/ql.max(Q)*100)\n",
    "\n",
    "# DONE: get maximum value from Q-Learning Matrix\n",
    "normed_Q = Q/ql.max(Q)*100\n",
    "max_location = np.where(normed_Q==normed_Q.max())\n",
    "print(\"\\nmax value located at\",max_location)\n",
    "max_config = max_location[0][0]\n",
    "max_algorithm = max_location[1][0]\n",
    "final_feats = bin_to_features(max_config)\n",
    "print(f\"\\nUsing algorithm {ALGORITHMS[max_algorithm]} and {final_feats}, max value is:\",normed_Q[max_config,max_algorithm])\n",
    "#DONE: print(f\"Selected features:\")\n",
    "\n",
    "# DONE: get final cluster labels\n",
    "cluster_labels = algorithm_prep(max_config, max_algorithm, 1)\n",
    "\n",
    "# DONE: match data in clusters to IP addresses\n",
    "labelled_data = data.copy()\n",
    "labelled_data['cluster'] = cluster_labels\n",
    "\n",
    "# DONE: return what IPs are likely anomalous\n",
    "# see what clusters have < 5% of the data\n",
    "# get unique values in cluster column\n",
    "num_clusters = labelled_data['cluster'].nunique()\n",
    "\n",
    "# for each unique value, get the count / len of data (aka percentage)\n",
    "# num_clusters = labelled_data['cluster'].nunique()\n",
    "cluster_array = labelled_data['cluster'].to_numpy()\n",
    "perc_values = np.unique(cluster_array,return_counts = True)[-1]\n",
    "percentages = perc_values / labelled_data.shape[0]\n",
    "\n",
    "# keep cluster values with % < 5\n",
    "idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "# output IPs within those selected clusters\n",
    "\n",
    "for i in anomalies['source_ip_main']: # replace with unique ID here\n",
    "    print(f\"\\nIP {i} is a potential anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd3a16",
   "metadata": {},
   "source": [
    "#### ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d77abc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296   1]\n",
      "[0.996633 0.003367]\n",
      "    device_name      source_ip  avg_bytes_sent  avg_bytes_received  \\\n",
      "142  Device-227  192.168.0.226          9973.5              4648.0   \n",
      "\n",
      "     avg_packets_transferred  avg_flow_duration  recent_tcp_flags  \\\n",
      "142                   303.75            7694.75                 2   \n",
      "\n",
      "     recent_protocol  avg_cpu_usage  avg_memory_usage  avg_disk_usage  \\\n",
      "142                1         29.545            58.905           58.96   \n",
      "\n",
      "     avg_uptime  cluster  \n",
      "142       426.0        1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# percentages = labelled_data['cluster'].value_counts().values\n",
    "# percentages = percentages / labelled_data.shape[0]\n",
    "# print(percentages)\n",
    "# print(labelled_data['cluster'].value_counts())\n",
    "cluster_array = labelled_data['cluster'].to_numpy()\n",
    "perc_vals = np.unique(cluster_array,return_counts = True)[-1]\n",
    "print(perc_vals)\n",
    "percentages = perc_vals / labelled_data.shape[0]\n",
    "print(percentages)\n",
    "idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a7393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 99.99998773, 100.        ,  99.99998111,  99.9512457 ,\n",
       "          99.9512457 ,  99.95124659,  99.99917226,  99.99991577,\n",
       "          99.9991523 ,  99.99991297,  99.95122339,  99.95121864]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(max_location)\n",
    "np.where(normed_Q==normed_Q.max())\n",
    "#normed_Q[normed_Q > 99.]\n",
    "#max_location = np.where(normed_Q==normed_Q.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66c348f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value located at (array([278]), array([0]))\n",
      "278\n",
      "Using algorithm K-Means and feature configuration 278, max value is: 100.0\n"
     ]
    }
   ],
   "source": [
    "normed_Q = Q/ql.max(Q)*100\n",
    "max_location = np.where(normed_Q==normed_Q.max())\n",
    "print(\"max value located at\",max_location)\n",
    "max_config = max_location[0][0]\n",
    "max_algorithm = ALGORITHMS[max_location[1][0]]\n",
    "print(f\"Using algorithm {max_algorithm} and feature configuration {max_config}, max value is:\",normed_Q[278,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa379bf0",
   "metadata": {},
   "source": [
    "## Additional Reference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b7bb23-50fc-4abd-8090-852424c7c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q  :\n",
      "[[  0.      0.      0.      0.    258.44    0.   ]\n",
      " [  0.      0.      0.    321.8     0.    207.752]\n",
      " [  0.      0.    500.    321.8     0.      0.   ]\n",
      " [  0.    258.44  401.      0.    258.44    0.   ]\n",
      " [207.752   0.      0.    321.8     0.      0.   ]\n",
      " [  0.    258.44    0.      0.      0.      0.   ]]\n",
      "Normed Q :\n",
      "[[  0.       0.       0.       0.      51.688    0.    ]\n",
      " [  0.       0.       0.      64.36     0.      41.5504]\n",
      " [  0.       0.     100.      64.36     0.       0.    ]\n",
      " [  0.      51.688   80.2      0.      51.688    0.    ]\n",
      " [ 41.5504   0.       0.      64.36     0.       0.    ]\n",
      " [  0.      51.688    0.       0.       0.       0.    ]]\n",
      "Concept Path\n",
      "-> A\n",
      "-> E\n",
      "-> D\n",
      "-> C\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Markov Decision Process (MDP) - The Bellman equations adapted to\n",
    "# Q Learning.Reinforcement Learning with the Q action-value(reward) function.\n",
    "# Copyright 2019 Denis Rothman MIT License. See LICENSE.\n",
    "import numpy as ql\n",
    "# R is The Reward Matrix for each state\n",
    "R = ql.matrix([ [0,0,0,0,1,0],\n",
    "\t\t            [0,0,0,1,0,1],\n",
    "\t\t            [0,0,100,1,0,0],\n",
    "\t             \t[0,1,1,0,1,0],\n",
    "\t\t            [1,0,0,1,0,0],\n",
    "\t\t            [0,1,0,0,0,0] ])\n",
    "\n",
    "# Q is the Learning Matrix in which rewards will be learned/stored\n",
    "Q = ql.matrix(ql.zeros([6,6]))\n",
    "\n",
    "\"\"\"##  The Learning rate or training penalty\"\"\"\n",
    "\n",
    "# Gamma : It's a form of penalty or uncertainty for learning\n",
    "# If the value is 1 , the rewards would be too high.\n",
    "# This way the system knows it is learning.\n",
    "gamma = 0.8\n",
    "\n",
    "\"\"\"## Initial State\"\"\"\n",
    "\n",
    "# agent_s_state. The agent the name of the system calculating\n",
    "# s is the state the agent is going from and s' the state it's going to\n",
    "# this state can be random or it can be chosen as long as the rest of the choices\n",
    "# are not determined. Randomness is part of this stochastic process\n",
    "agent_s_state = 5\n",
    "\n",
    "\"\"\"## The random choice of the next state\"\"\"\n",
    "\n",
    "# The possible \"a\" actions when the agent is in a given state\n",
    "def possible_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    possible_act = ql.where(current_state_row >0)[1]\n",
    "    return possible_act\n",
    "\n",
    "# Get available actions in the current state\n",
    "PossibleAction = possible_actions(agent_s_state)\n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def ActionChoice(available_actions_range):\n",
    "    if(sum(PossibleAction)>0):\n",
    "        next_action = int(ql.random.choice(PossibleAction,1))\n",
    "    if(sum(PossibleAction)<=0):\n",
    "        next_action = int(ql.random.choice(5,1))\n",
    "    return next_action\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = ActionChoice(PossibleAction)\n",
    "\n",
    "\"\"\"## The Bellman Equation\"\"\"\n",
    "\n",
    "# A version of the Bellman equation for reinforcement learning using the Q function\n",
    "# This reinforcement algorithm is a memoryless process\n",
    "# The transition function T from one state to another\n",
    "# is not in the equation below.  T is done by the random choice above\n",
    "\n",
    "def reward(current_state, action, gamma):\n",
    "    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n",
    "\n",
    "    if Max_State.shape[0] > 1:\n",
    "        Max_State = int(ql.random.choice(Max_State, size = 1))\n",
    "    else:\n",
    "        Max_State = int(Max_State)\n",
    "    MaxValue = Q[action, Max_State]\n",
    "    \n",
    "    # The Bellman MDP based Q function\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * MaxValue\n",
    "\n",
    "# Rewarding Q matrix\n",
    "reward(agent_s_state,action,gamma)\n",
    "\n",
    "\"\"\"## Running the training episodes randomly\"\"\"\n",
    "\n",
    "# Learning over n iterations depending on the convergence of the system\n",
    "# A convergence function can replace the systematic repeating of the process\n",
    "# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n",
    "# previous episode\n",
    "for i in range(50000):\n",
    "    current_state = ql.random.randint(0, int(Q.shape[0]))\n",
    "    PossibleAction = possible_actions(current_state)\n",
    "    action = ActionChoice(PossibleAction)\n",
    "    reward(current_state,action,gamma)\n",
    "    \n",
    "# Displaying Q before the norm of Q phase\n",
    "print(\"Q  :\")\n",
    "print(Q)\n",
    "\n",
    "# Norm of Q\n",
    "print(\"Normed Q :\")\n",
    "print(Q/ql.max(Q)*100)\n",
    "\n",
    "\"\"\"# Improving the program by introducing a decision-making process\"\"\"\n",
    "nextc=-1\n",
    "nextci=-1\n",
    "conceptcode=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "origin=int(input(\"index number origin(A=0,B=1,C=2,D=3,E=4,F=5): \"))\n",
    "print(\"Concept Path\")\n",
    "print(\"->\",conceptcode[int(origin)])\n",
    "for se in range(0,6):\n",
    "    if(se==0):\n",
    "        po=origin\n",
    "    if(se>0):\n",
    "        po=nextci\n",
    "        #print(\"se:\",se,\"po:\",po)\n",
    "    for ci in range(0,6):\n",
    "        maxc=Q[po,ci]\n",
    "        #print(maxc,nextc)\n",
    "        if(maxc>=nextc):\n",
    "            nextc=maxc\n",
    "            nextci=ci\n",
    "            #print(\"next c\",nextc)\n",
    "    if(nextci==po):\n",
    "        break;\n",
    "    #print(\"present origin\",po,\"next c\",nextci,\" \",nextc,\" \",conceptcode[int(nextci)])\n",
    "    print(\"->\",conceptcode[int(nextci)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
