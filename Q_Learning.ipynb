{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c2e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c02a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pip in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (24.3.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7652df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\python312\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Downloading fonttools-4.55.0-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aaaimeeeelll\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.8 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Python312\\\\share'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d895795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn-extra\n",
      "  Downloading scikit-learn-extra-0.3.0.tar.gz (818 kB)\n",
      "     ---------------------------------------- 0.0/819.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 819.0/819.0 kB 9.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\python312\\lib\\site-packages (from scikit-learn-extra) (2.1.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\python312\\lib\\site-packages (from scikit-learn-extra) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\python312\\lib\\site-packages (from scikit-learn-extra) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.5.0)\n",
      "Building wheels for collected packages: scikit-learn-extra\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml): started\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml): finished with status 'error'\n",
      "Failed to build scikit-learn-extra\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for scikit-learn-extra (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [60 lines of output]\n",
      "      E:\\Temp\\pip-build-env-hm3epjds\\overlay\\Lib\\site-packages\\setuptools\\dist.py:488: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2025-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self.warn_dash_deprecation(opt, section)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\benchmarks\n",
      "      copying benchmarks\\bench_rbfsampler_fastfood.py -> build\\lib.win-amd64-cpython-312\\benchmarks\n",
      "      copying benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\benchmarks\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\n",
      "      copying sklearn_extra\\_version.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\n",
      "      copying sklearn_extra\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\n",
      "      creating build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\eigenpro_plot_mnist.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\eigenpro_plot_noisy_mnist.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\eigenpro_plot_synthetic.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\robust_plot_synthetic.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      copying benchmarks\\_bench\\__init__.py -> build\\lib.win-amd64-cpython-312\\benchmarks\\_bench\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      copying sklearn_extra\\cluster\\_commonnn.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      copying sklearn_extra\\cluster\\_k_medoids.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      copying sklearn_extra\\cluster\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      copying sklearn_extra\\kernel_approximation\\test_fastfood.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      copying sklearn_extra\\kernel_approximation\\_fastfood.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      copying sklearn_extra\\kernel_approximation\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\n",
      "      copying sklearn_extra\\kernel_methods\\_eigenpro.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\n",
      "      copying sklearn_extra\\kernel_methods\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      copying sklearn_extra\\robust\\mean_estimators.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      copying sklearn_extra\\robust\\robust_weighted_estimator.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      copying sklearn_extra\\robust\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\robust\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\tests\n",
      "      copying sklearn_extra\\tests\\test_common.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\tests\n",
      "      copying sklearn_extra\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\tests\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      copying sklearn_extra\\cluster\\tests\\test_commonnn.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      copying sklearn_extra\\cluster\\tests\\test_k_medoids.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      copying sklearn_extra\\cluster\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\cluster\\tests\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\\tests\n",
      "      copying sklearn_extra\\kernel_approximation\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_approximation\\tests\n",
      "      creating build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\\tests\n",
      "      copying sklearn_extra\\kernel_methods\\tests\\test_eigenpro.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\\tests\n",
      "      copying sklearn_extra\\kernel_methods\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\sklearn_extra\\kernel_methods\\tests\n",
      "      running build_ext\n",
      "      building 'sklearn_extra.utils._cyfht' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-learn-extra\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-learn-extra)\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f00d1a",
   "metadata": {},
   "source": [
    "### Helper Functions: Algorithms & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5c78b",
   "metadata": {},
   "source": [
    "**Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8fe4e",
   "metadata": {},
   "source": [
    "DONE: add a \"mode\" argument to each algorithm that, if mode = 1 the cluster labelling is output and if mode = 0 the silhouette coefficient is output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfd14e3-20b2-4404-a2ea-ff8f7e5aa83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for KMeans\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def kmeans_clustering(samples,mode,  n_clusters=2, max_iter=300):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on the input samples\n",
    "    \n",
    "    Parameters:\n",
    "        samples: array-like, shape (n_samples, n_features)\n",
    "        n_clusters: int, number of clusters (default=2)\n",
    "        max_iter: int, maximum iterations (default=300)\n",
    "    \n",
    "    Returns:\n",
    "        silhouette_coef: silhouette coefficient score\n",
    "    \"\"\"\n",
    "    k_means = KMeans(n_clusters=n_clusters, max_iter=max_iter)\n",
    "    k_means.fit(samples)\n",
    "    if mode == 0:\n",
    "        try:\n",
    "            silhouette_coef = silhouette_score(samples, k_means.labels_, metric='euclidean')\n",
    "        except ValueError:\n",
    "            silhouette_coef = 0  # Assigning lowest score if clustering fails\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return k_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d63cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EM Clustering Code\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def em_clustering(selected_features, mode, n_clusters=2):\n",
    "    \"\"\"\n",
    "    Perform EM Clustering on selected features and return silhouette score.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Silhouette score of the clustering (-1 if clustering fails)\n",
    "    \"\"\"\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize and fit the EM model\n",
    "    em_model = GaussianMixture(\n",
    "        n_components=n_clusters,\n",
    "        random_state=0, #THOUGHTS: We can improve this later to have an array of seeds to select from to observe variations\n",
    "        n_init=10  # Multiple initializations to avoid local optima\n",
    "    )\n",
    "    \n",
    "   \n",
    "    try:\n",
    "        # Fit the model and get cluster assignments\n",
    "        em_model.fit(X_scaled)\n",
    "        labels = em_model.predict(X_scaled)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except Exception as e:\n",
    "        #print(f\"Clustering failed: {str(e)}\")\n",
    "        silhouette_coef = 0  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9944db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Detection method: \n",
    "# I put 'optimization part' in 'DBSCAN_Optimization_Code.ipynb' file. \n",
    "# We can use optimization after initial run to do a comparison and analysis in our paper to show improvements.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TO-DO: keep -1 --> they will be its own cluster\n",
    "def dbscan_clustering(selected_features, mode, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on selected features\n",
    "    \n",
    "    Parameters:\n",
    "    selected_features : pandas DataFrame\n",
    "        The features selected for clustering\n",
    "    eps : float\n",
    "        The maximum distance between two samples for them to be considered neighbors\n",
    "    min_samples : int\n",
    "        The number of samples in a neighborhood for a point to be considered a core point\n",
    "        \n",
    "    Returns:\n",
    "    float : silhouette coefficient\n",
    "    dict : additional clustering information\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize and fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Get number of clusters (excluding noise points which are labeled -1, K Medoids does not have noise points)\n",
    "    n_clusters = len(set(labels))\n",
    "\n",
    "    if -1 in labels:\n",
    "        labels[labels == -1] = n_clusters - 1\n",
    "    \n",
    "    # calculate silhouette score if more than one cluster and  noise points\n",
    "    if n_clusters > 1:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    else:\n",
    "        silhouette_coef = 0  # Assign lowest score if clustering fails\n",
    "\n",
    "    \n",
    "    # NOTE: -- Uncomment when we analyze and optimize ---- Additional clustering information\n",
    "    # info = {\n",
    "    #     'n_clusters': n_clusters,\n",
    "    #     'n_noise': list(labels).count(-1),\n",
    "    #     'labels': labels,\n",
    "    #     'cluster_sizes': pd.Series(labels).value_counts().to_dict()\n",
    "    # }\n",
    "    \n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16776843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for K Medoids\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def kmedoids_clustering(selected_features, mode, n_clusters=2):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "     # Initialize and fit the K-Medoids model\n",
    "    kmedoids = KMedoids(n_clusters=n_clusters, method='pam', max_iter=1000, random_state=0)\n",
    "    labels = kmedoids.fit_predict(X_scaled)\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except ValueError:\n",
    "        silhouette_coef = 0  # Assigning lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078cfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for Mean Shift\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def meanshift_clustering(selected_features, mode, bandwidth=None):\n",
    "    # Filter the selected features\n",
    "    X = selected_features\n",
    "    \n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize and fit the Mean Shift model\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    labels = meanshift.fit_predict(X_scaled)\n",
    "\n",
    "    # Check the number of clusters determined \n",
    "    n_clusters = len(np.unique(labels))\n",
    "    #print(f\"Number of clusters found: {n_clusters}\")\n",
    "    \n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    try:\n",
    "        silhouette_coef = silhouette_score(X_scaled, labels)\n",
    "    except ValueError:\n",
    "        silhouette_coef = 0  # Assign lowest score if clustering fails\n",
    "    if mode == 0:\n",
    "        return silhouette_coef\n",
    "    if mode == 1:\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946bd80",
   "metadata": {},
   "source": [
    "**Clustering & Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740d094",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed306bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "# idx = []\n",
    "# anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "# print(len(anomalies))\n",
    "perc_values = labelled_data['cluster'].value_counts(ascending=True)\n",
    "print(perc_values)\n",
    "print(\"flipped:\",perc_values.values)\n",
    "print(percentages)\n",
    "idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "print(idx)\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "print(len(anomalies))\n",
    "perc_values = labelled_data['cluster'].value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7245129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FEATURES = {0: 'avg_bytes_sent', 1: 'avg_bytes_received', 2: 'avg_packets_transferred', \n",
    "  3: 'avg_flow_duration', 4: 'recent_tcp_flags', 5: 'recent_protocol', 6: 'avg_cpu_usage', \n",
    "  7: 'avg_memory_usage', 8: 'avg_disk_usage', 9: 'avg_uptime'}\n",
    "\n",
    "data = pd.read_csv(\"joined_quantitative_data.csv\")\n",
    "\n",
    "ALGORITHMS = {4: 'K-Means', 1: 'Mean Shift', 2: 'K-Mediods', 3: 'EM Clustering', 0: 'DBSCAN Clustering'}\n",
    "NUM_ALG = len(ALGORITHMS)\n",
    "original_features = data.iloc[:, 2:]\n",
    "ips = data['source_ip']\n",
    "#print(original_features.columns)\n",
    "#print(ips.head(10))\n",
    "\n",
    "def algorithm_prep(state, action, mode):\n",
    "  # convert state to binary\n",
    "  state_bin = bin(state)\n",
    "  #print(state_bin)\n",
    "  state_bin_arr = np.array([b for b in state_bin[2:]])\n",
    "  # pad with zeros\n",
    "  diff = 10 - len(state_bin_arr)\n",
    "  padded_arr = np.insert(state_bin_arr, 0, ['0' for i in range(diff)])\n",
    "  #(padded_arr)\n",
    "  # identify which indexes are 1\n",
    "  idx = (np.where(padded_arr == '1')[0]).tolist()\n",
    "  #print(idx)\n",
    "  # select feature headings\n",
    "  selected_features = original_features.iloc[:,idx]\n",
    "  #print(selected_features.head(10))\n",
    "  # select algorithm\n",
    "  # algo = action\n",
    "  # prep correct data - done\n",
    "  \n",
    "  # call algorithm function\n",
    "  out = None\n",
    "  #print('algorithm:',ALGORITHMS[action])\n",
    "\n",
    "  # if mode = 0, output is the silhouette coefficient\n",
    "  # if mode = 1, output is the cluster labelling\n",
    "  match action:   \n",
    "    case 0:\n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = dbscan_clustering(selected_features, mode)\n",
    "    case 1: \n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = meanshift_clustering(selected_features, mode)\n",
    "    case 2:\n",
    "      #print('algorithm:',ALGORITHMS[action])\n",
    "      out = kmedoids_clustering(selected_features, mode)\n",
    "    case 3: \n",
    "      out = em_clustering(selected_features, mode)\n",
    "    case 4:\n",
    "      out = kmeans_clustering(selected_features, mode)\n",
    "      \n",
    "  # return silhouette from algorithm function\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d2115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_features(state):\n",
    "  state_bin = bin(state)\n",
    "  #print(state_bin)\n",
    "  state_bin_arr = np.array([b for b in state_bin[2:]])\n",
    "  # pad with zeros\n",
    "  diff = 10 - len(state_bin_arr)\n",
    "  padded_arr = np.insert(state_bin_arr, 0, ['0' for i in range(diff)])\n",
    "  #(padded_arr)\n",
    "  # identify which indexes are 1\n",
    "  idx = (np.where(padded_arr == '1')[0]).tolist()\n",
    "  #print(idx)\n",
    "  # select feature headings\n",
    "  selected_features = original_features.iloc[:,idx]\n",
    "  features = selected_features.columns.tolist()\n",
    "  print(\"Features Used:\", features)\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87ad63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used: ['recent_tcp_flags', 'recent_protocol']\n"
     ]
    }
   ],
   "source": [
    "bin_to_features(48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148db5c",
   "metadata": {},
   "source": [
    "##### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e31d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "[1]\n",
      "IP 192.168.0.248 is a potential anomaly\n",
      "IP 192.168.0.78 is a potential anomaly\n"
     ]
    }
   ],
   "source": [
    "label_test = algorithm_prep(256, 4,1)\n",
    "labelled_data = data.copy()\n",
    "labelled_data['cluster'] = label_test\n",
    "labelled_data.head(10)\n",
    "\n",
    "num_clusters = labelled_data['cluster'].nunique()\n",
    "vals = labelled_data['cluster'].value_counts().values\n",
    "print(labelled_data.shape[0])\n",
    "vals = vals / labelled_data.shape[0]\n",
    "idx = (np.where(vals <= 0.1)[0]).tolist()\n",
    "print(idx)\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "#print(anomalies)\n",
    "for i in anomalies['source_ip']:\n",
    "    print(f\"IP {i} is a potential anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43bc6be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labelled_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabelled_data\u001b[49m[labelled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labelled_data' is not defined"
     ]
    }
   ],
   "source": [
    "labelled_data[labelled_data['cluster']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045098f",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beedbfe-7d34-401e-b99f-e48e93937a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:297: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q  :\n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.63071706 0.63071706 0.63040802]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.13145184 0.         1.81181496]\n",
      " [0.         0.69277059 0.1199422  0.7984389  1.81181623]\n",
      " [0.         0.         0.11100989 0.78066054 1.81181495]]\n",
      "Normed Q :\n",
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.         34.31096091 34.31096091 34.29414902]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " ...\n",
      " [ 0.          0.          7.15097043  0.         98.56259762]\n",
      " [ 0.         37.68666811  6.52484694 43.43501649 98.56266685]\n",
      " [ 0.          0.          6.03892973 42.46787466 98.56259722]]\n",
      "\n",
      "max value located at (array([32, 48]), array([4, 4]))\n",
      "\n",
      "Using algorithm K-Means and feature configuration 32, max value is: 100.0\n",
      "\n",
      "IP 192.168.0.0 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.1 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.10 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.100 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.101 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.102 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.103 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.104 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.105 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.106 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.107 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.108 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.109 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.11 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.110 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.111 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.112 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.113 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.114 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.115 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.116 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.117 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.118 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.119 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.12 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.120 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.121 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.122 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.123 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.124 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.125 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.126 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.127 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.128 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.129 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.13 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.130 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.131 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.132 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.133 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.134 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.135 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.136 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.137 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.138 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.139 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.14 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.140 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.141 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.142 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.143 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.144 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.145 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.146 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.147 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.148 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.149 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.15 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.150 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.151 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.152 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.153 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.154 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.155 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.156 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.157 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.158 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.159 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.16 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.160 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.161 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.162 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.163 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.164 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.165 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.166 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.167 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.168 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.169 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.17 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.170 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.171 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.172 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.173 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.174 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.175 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.176 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.177 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.178 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.179 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.18 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.180 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.181 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.182 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.183 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.184 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.185 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.186 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.187 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.188 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.189 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.19 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.190 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.191 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.192 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.193 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.194 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.195 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.196 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.197 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.198 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.199 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.2 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.20 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.200 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.201 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.202 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.203 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.204 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.205 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.206 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.207 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.208 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.209 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.21 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.210 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.211 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.212 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.213 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.214 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.215 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.216 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.217 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.218 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.219 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.22 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.220 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.221 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.222 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.223 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.224 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.225 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.227 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.228 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.229 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.23 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.230 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.231 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.232 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.233 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.234 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.235 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.236 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.237 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.238 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.239 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.24 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.240 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.241 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.242 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.243 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.244 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.245 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.246 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.247 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.248 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.249 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.25 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.250 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.251 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.252 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.253 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.254 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.26 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.27 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.28 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.29 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.3 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.30 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.31 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.32 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.33 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.34 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.35 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.36 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.37 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.38 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.39 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.4 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.40 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.41 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.42 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.43 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.44 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.45 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.46 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.47 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.48 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.49 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.5 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.50 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.51 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.52 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.53 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.54 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.55 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.56 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.57 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.58 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.59 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.6 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.60 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.61 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.62 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.63 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.64 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.65 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.66 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.67 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.68 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.69 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.7 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.70 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.71 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.72 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.73 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.74 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.75 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.76 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.77 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.78 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.79 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.8 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.80 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.81 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.82 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.83 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.84 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.85 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.86 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.87 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.88 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.89 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.9 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.90 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.91 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.92 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.93 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.94 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.95 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.96 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.97 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.98 is a potential anomaly\n",
      "\n",
      "IP 192.168.0.99 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.0 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.1 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.10 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.11 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.12 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.13 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.14 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.15 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.16 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.17 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.18 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.19 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.2 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.20 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.21 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.22 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.23 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.24 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.25 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.26 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.27 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.28 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.29 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.3 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.30 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.31 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.32 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.33 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.34 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.35 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.36 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.37 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.38 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.39 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.4 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.40 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.41 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.5 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.6 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.7 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.8 is a potential anomaly\n",
      "\n",
      "IP 192.168.1.9 is a potential anomaly\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process (MDP) - The Bellman equations adapted to\n",
    "# Q Learning.Reinforcement Learning with the Q action-value(reward) function.\n",
    "# Copyright 2018 Denis Rothman MIT License. See LICENSE.\n",
    "import numpy as ql\n",
    "# R is The Reward Matrix for each state\n",
    "# 1024 configurations of the 10 features --> 2^10\n",
    "# 5 algorithms\n",
    "R = ql.matrix(ql.zeros([1024,5]))\n",
    "\n",
    "# Q is the Learning Matrix in which rewards will be learned/stored\n",
    "Q = ql.matrix(ql.zeros([1024,5]))\n",
    "\n",
    "# Gamma : It's a form of penalty or uncertainty for learning\n",
    "# If the value is 1 , the rewards would be too high.\n",
    "# This way the system knows it is learning.\n",
    "gamma = 0.8\n",
    "\n",
    "# agent_s_state. The agent the name of the system calculating\n",
    "# s is the state the agent is going from and s' the state it's going to\n",
    "# this state can be random or it can be chosen as long as the rest of the choices\n",
    "# are not determined. Randomness is part of this stochastic process\n",
    "# 1) DONE: decide if starting state is random or a specific state\n",
    "agent_s_state = 1\n",
    "\n",
    "# The possible \"a\" actions when the agent is in a given state\n",
    "def possible_actions(state):\n",
    "    # 2) DONE: we should check Q, not R because R is never modified\n",
    "    current_state_row = Q[state,]\n",
    "    # 3) DONE: this should pick valid actions based on what we have not visited\n",
    "    possible_act = ql.where(current_state_row == 0)[1]\n",
    "    return possible_act\n",
    "\n",
    "# Get available actions in the current state\n",
    "PossibleAction = possible_actions(agent_s_state)\n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def ActionChoice(available_actions_range):\n",
    "    if(sum(PossibleAction)>0):\n",
    "        next_action = int(ql.random.choice(PossibleAction,1)[0])\n",
    "    if(sum(PossibleAction)<=0):\n",
    "        next_action = int(np.random.choice(NUM_ALG,1)[0])\n",
    "    return next_action\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = ActionChoice(PossibleAction)\n",
    "\n",
    "# A version of Bellman's equation for reinforcement learning using the Q function\n",
    "# This reinforcement algorithm is a memoryless process\n",
    "# The transition function T from one state to another\n",
    "# is not in the equation below.  T is done by the random choice above\n",
    "\n",
    "def reward(current_state, action, gamma):\n",
    "    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n",
    "\n",
    "    if Max_State.shape[0] > 1:\n",
    "        Max_State = int(ql.random.choice(Max_State, size = 1)[0])\n",
    "    else:\n",
    "        Max_State = int(Max_State[0])\n",
    "\n",
    "    # 5) DONE: we think this is a typo and action/Max_State should be switched. \n",
    "    # MaxValue = Q[action, Max_State]\n",
    "    MaxValue = Q[Max_State, action]\n",
    "\n",
    "    # 6) DONE: call function to run ML algorithm using the value of action. this will\n",
    "    # run the algorithm using the features from current_state, create clusters,\n",
    "    # and calculate the silhouette value.\n",
    "    silhouette_co = algorithm_prep(current_state, action, 0) \n",
    "    \n",
    "    # Bellman's MDP based Q function\n",
    "    # 7) DONE: instead of getting a value from R, we add the silhouette value to gamma * MaxValue\n",
    "    # Q[current_state, action] = R[current_state, action] + gamma * MaxValue\n",
    "    Q[current_state, action] = silhouette_co + gamma * MaxValue\n",
    "\n",
    "\n",
    "# Rewarding Q matrix\n",
    "reward(agent_s_state,action,gamma)\n",
    "\n",
    "\n",
    "# Leraning over n iterations depending on the convergence of the system\n",
    "# A convergence function can replace the systematic repeating of the process\n",
    "# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n",
    "# previous episode\n",
    "for i in range(6000):\n",
    "    # select a random new state (configuration of features)\n",
    "    current_state = ql.random.randint(1, int(Q.shape[0]))\n",
    "    PossibleAction = possible_actions(current_state)\n",
    "    action = ActionChoice(PossibleAction)\n",
    "    reward(current_state,action,gamma)\n",
    "    \n",
    "# Displaying Q before the norm of Q phase\n",
    "print(\"Q  :\")\n",
    "print(Q)\n",
    "\n",
    "# Norm of Q\n",
    "print(\"Normed Q :\")\n",
    "print(Q/ql.max(Q)*100)\n",
    "\n",
    "# DONE: get maximum value from Q-Learning Matrix\n",
    "normed_Q = Q/ql.max(Q)*100\n",
    "max_location = np.where(normed_Q==normed_Q.max())\n",
    "print(\"\\nmax value located at\",max_location)\n",
    "max_config = max_location[0][0]\n",
    "max_algorithm = max_location[1][0]\n",
    "final_feats = bin_to_features(max_config)\n",
    "print(f\"\\nUsing algorithm {ALGORITHMS[max_algorithm]} and {final_feats}, max value is:\",normed_Q[max_config,max_algorithm])\n",
    "#TO-DO: print(f\"Selected features:\", )\n",
    "\n",
    "# DONE: get final cluster labels\n",
    "cluster_labels = algorithm_prep(max_config, max_algorithm, 1)\n",
    "\n",
    "# DONE: match data in clusters to IP addresses\n",
    "labelled_data = data.copy()\n",
    "labelled_data['cluster'] = cluster_labels\n",
    "\n",
    "# TO-DO: return what IPs are likely anomalous\n",
    "# see what clusters have < 5% of the data\n",
    "# get unique values in cluster column\n",
    "num_clusters = labelled_data['cluster'].nunique()\n",
    "\n",
    "# for each unique value, get the count / len of data (aka percentage)\n",
    "# num_clusters = labelled_data['cluster'].nunique()\n",
    "cluster_array = labelled_data['cluster'].to_numpy()\n",
    "perc_values = np.unique(cluster_array,return_counts = True)[-1]\n",
    "percentages = perc_values / labelled_data.shape[0]\n",
    "\n",
    "# keep cluster values with % < 5\n",
    "idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "# output IPs within those selected clusters\n",
    "#print(f\"There are {len(an)}\")\n",
    "for i in anomalies['source_ip']:\n",
    "    print(f\"\\nIP {i} is a potential anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd3a16",
   "metadata": {},
   "source": [
    "#### ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d77abc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296   1]\n",
      "[0.996633 0.003367]\n",
      "    device_name      source_ip  avg_bytes_sent  avg_bytes_received  \\\n",
      "142  Device-227  192.168.0.226          9973.5              4648.0   \n",
      "\n",
      "     avg_packets_transferred  avg_flow_duration  recent_tcp_flags  \\\n",
      "142                   303.75            7694.75                 2   \n",
      "\n",
      "     recent_protocol  avg_cpu_usage  avg_memory_usage  avg_disk_usage  \\\n",
      "142                1         29.545            58.905           58.96   \n",
      "\n",
      "     avg_uptime  cluster  \n",
      "142       426.0        1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# percentages = labelled_data['cluster'].value_counts().values\n",
    "# percentages = percentages / labelled_data.shape[0]\n",
    "# print(percentages)\n",
    "# print(labelled_data['cluster'].value_counts())\n",
    "cluster_array = labelled_data['cluster'].to_numpy()\n",
    "perc_vals = np.unique(cluster_array,return_counts = True)[-1]\n",
    "print(perc_vals)\n",
    "percentages = perc_vals / labelled_data.shape[0]\n",
    "print(percentages)\n",
    "idx = (np.where(percentages <= 0.1)[0]).tolist()\n",
    "anomalies = labelled_data.loc[labelled_data['cluster'].isin(idx)]\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a7393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 99.99998773, 100.        ,  99.99998111,  99.9512457 ,\n",
       "          99.9512457 ,  99.95124659,  99.99917226,  99.99991577,\n",
       "          99.9991523 ,  99.99991297,  99.95122339,  99.95121864]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(max_location)\n",
    "np.where(normed_Q==normed_Q.max())\n",
    "#normed_Q[normed_Q > 99.]\n",
    "#max_location = np.where(normed_Q==normed_Q.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66c348f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value located at (array([278]), array([0]))\n",
      "278\n",
      "Using algorithm K-Means and feature configuration 278, max value is: 100.0\n"
     ]
    }
   ],
   "source": [
    "normed_Q = Q/ql.max(Q)*100\n",
    "max_location = np.where(normed_Q==normed_Q.max())\n",
    "print(\"max value located at\",max_location)\n",
    "max_config = max_location[0][0]\n",
    "max_algorithm = ALGORITHMS[max_location[1][0]]\n",
    "print(f\"Using algorithm {max_algorithm} and feature configuration {max_config}, max value is:\",normed_Q[278,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa379bf0",
   "metadata": {},
   "source": [
    "## Additional Reference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b7bb23-50fc-4abd-8090-852424c7c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q  :\n",
      "[[  0.      0.      0.      0.    258.44    0.   ]\n",
      " [  0.      0.      0.    321.8     0.    207.752]\n",
      " [  0.      0.    500.    321.8     0.      0.   ]\n",
      " [  0.    258.44  401.      0.    258.44    0.   ]\n",
      " [207.752   0.      0.    321.8     0.      0.   ]\n",
      " [  0.    258.44    0.      0.      0.      0.   ]]\n",
      "Normed Q :\n",
      "[[  0.       0.       0.       0.      51.688    0.    ]\n",
      " [  0.       0.       0.      64.36     0.      41.5504]\n",
      " [  0.       0.     100.      64.36     0.       0.    ]\n",
      " [  0.      51.688   80.2      0.      51.688    0.    ]\n",
      " [ 41.5504   0.       0.      64.36     0.       0.    ]\n",
      " [  0.      51.688    0.       0.       0.       0.    ]]\n",
      "Concept Path\n",
      "-> A\n",
      "-> E\n",
      "-> D\n",
      "-> C\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Markov Decision Process (MDP) - The Bellman equations adapted to\n",
    "# Q Learning.Reinforcement Learning with the Q action-value(reward) function.\n",
    "# Copyright 2019 Denis Rothman MIT License. See LICENSE.\n",
    "import numpy as ql\n",
    "# R is The Reward Matrix for each state\n",
    "R = ql.matrix([ [0,0,0,0,1,0],\n",
    "\t\t            [0,0,0,1,0,1],\n",
    "\t\t            [0,0,100,1,0,0],\n",
    "\t             \t[0,1,1,0,1,0],\n",
    "\t\t            [1,0,0,1,0,0],\n",
    "\t\t            [0,1,0,0,0,0] ])\n",
    "\n",
    "# Q is the Learning Matrix in which rewards will be learned/stored\n",
    "Q = ql.matrix(ql.zeros([6,6]))\n",
    "\n",
    "\"\"\"##  The Learning rate or training penalty\"\"\"\n",
    "\n",
    "# Gamma : It's a form of penalty or uncertainty for learning\n",
    "# If the value is 1 , the rewards would be too high.\n",
    "# This way the system knows it is learning.\n",
    "gamma = 0.8\n",
    "\n",
    "\"\"\"## Initial State\"\"\"\n",
    "\n",
    "# agent_s_state. The agent the name of the system calculating\n",
    "# s is the state the agent is going from and s' the state it's going to\n",
    "# this state can be random or it can be chosen as long as the rest of the choices\n",
    "# are not determined. Randomness is part of this stochastic process\n",
    "agent_s_state = 5\n",
    "\n",
    "\"\"\"## The random choice of the next state\"\"\"\n",
    "\n",
    "# The possible \"a\" actions when the agent is in a given state\n",
    "def possible_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    possible_act = ql.where(current_state_row >0)[1]\n",
    "    return possible_act\n",
    "\n",
    "# Get available actions in the current state\n",
    "PossibleAction = possible_actions(agent_s_state)\n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def ActionChoice(available_actions_range):\n",
    "    if(sum(PossibleAction)>0):\n",
    "        next_action = int(ql.random.choice(PossibleAction,1))\n",
    "    if(sum(PossibleAction)<=0):\n",
    "        next_action = int(ql.random.choice(5,1))\n",
    "    return next_action\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = ActionChoice(PossibleAction)\n",
    "\n",
    "\"\"\"## The Bellman Equation\"\"\"\n",
    "\n",
    "# A version of the Bellman equation for reinforcement learning using the Q function\n",
    "# This reinforcement algorithm is a memoryless process\n",
    "# The transition function T from one state to another\n",
    "# is not in the equation below.  T is done by the random choice above\n",
    "\n",
    "def reward(current_state, action, gamma):\n",
    "    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n",
    "\n",
    "    if Max_State.shape[0] > 1:\n",
    "        Max_State = int(ql.random.choice(Max_State, size = 1))\n",
    "    else:\n",
    "        Max_State = int(Max_State)\n",
    "    MaxValue = Q[action, Max_State]\n",
    "    \n",
    "    # The Bellman MDP based Q function\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * MaxValue\n",
    "\n",
    "# Rewarding Q matrix\n",
    "reward(agent_s_state,action,gamma)\n",
    "\n",
    "\"\"\"## Running the training episodes randomly\"\"\"\n",
    "\n",
    "# Learning over n iterations depending on the convergence of the system\n",
    "# A convergence function can replace the systematic repeating of the process\n",
    "# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n",
    "# previous episode\n",
    "for i in range(50000):\n",
    "    current_state = ql.random.randint(0, int(Q.shape[0]))\n",
    "    PossibleAction = possible_actions(current_state)\n",
    "    action = ActionChoice(PossibleAction)\n",
    "    reward(current_state,action,gamma)\n",
    "    \n",
    "# Displaying Q before the norm of Q phase\n",
    "print(\"Q  :\")\n",
    "print(Q)\n",
    "\n",
    "# Norm of Q\n",
    "print(\"Normed Q :\")\n",
    "print(Q/ql.max(Q)*100)\n",
    "\n",
    "\"\"\"# Improving the program by introducing a decision-making process\"\"\"\n",
    "nextc=-1\n",
    "nextci=-1\n",
    "conceptcode=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "origin=int(input(\"index number origin(A=0,B=1,C=2,D=3,E=4,F=5): \"))\n",
    "print(\"Concept Path\")\n",
    "print(\"->\",conceptcode[int(origin)])\n",
    "for se in range(0,6):\n",
    "    if(se==0):\n",
    "        po=origin\n",
    "    if(se>0):\n",
    "        po=nextci\n",
    "        #print(\"se:\",se,\"po:\",po)\n",
    "    for ci in range(0,6):\n",
    "        maxc=Q[po,ci]\n",
    "        #print(maxc,nextc)\n",
    "        if(maxc>=nextc):\n",
    "            nextc=maxc\n",
    "            nextci=ci\n",
    "            #print(\"next c\",nextc)\n",
    "    if(nextci==po):\n",
    "        break;\n",
    "    #print(\"present origin\",po,\"next c\",nextci,\" \",nextc,\" \",conceptcode[int(nextci)])\n",
    "    print(\"->\",conceptcode[int(nextci)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
